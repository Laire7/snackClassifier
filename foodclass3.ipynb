{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e57b2d8b-f1b4-4fac-b068-fbca8adba139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3ddfb304-1d86-43c8-be1e-dbd06e347e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# scaler = GradScaler()\n",
    "\n",
    "# for inputs, labels in train_loader:\n",
    "#     with autocast():\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#     scaler.scale(loss).backward()\n",
    "#     scaler.step(optimizer)\n",
    "#     scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "df14bfc9-14b7-4ab9-855c-36b96ec19cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d4d30360-4f62-4c68-9d5e-717f3b5c28dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nvidia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnvidia\u001b[49m\u001b[38;5;241m-\u001b[39msmi\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nvidia' is not defined"
     ]
    }
   ],
   "source": [
    "# nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98279c9e-b81f-441b-bfaa-82dd4dc51192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataPath = os.path.join(os.getcwd(), 'data', 'snack')\n",
    "dataPath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a607fae-a1a3-4368-a75e-9445ef540d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "snackType = os.listdir(dataPath)\n",
    "print(snackType)\n",
    "snackFiles = os.listdir(os.path.join(dataPath, snackType[0]))\n",
    "print(snackFiles)\n",
    "num = random.randint(0,len(snackType[0]))\n",
    "print(snackFiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713d0db-7580-407c-93a3-03a428febaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRatio = 0.6\n",
    "validRatio = 0.2\n",
    "testRatio = 1 - trainRatio - validRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a38a0-ed5a-461b-94d6-19422425e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for snack in snackType:\n",
    "    snackPath = os.path.join(dataPath, snack)\n",
    "    files = os.listdir(snackPath)\n",
    "    \n",
    "    trainCount = int(trainRatio * len(files))\n",
    "    validCount = int(validRatio * len(files))\n",
    "    testCount = len(files) - trainCount - validCount\n",
    "    print(f'trainCount:{trainCount} validCount:{validCount} testCount:{testCount}')\n",
    "\n",
    "    print(f'files {len(files)}')\n",
    "    shuffleFiles = random.sample(files, len(files))\n",
    "    print(f'shuffleFiles {len(shuffleFiles)}')\n",
    "\n",
    "    # shutil.rmtree(os.path.join(snackPath,'train'))\n",
    "    # shutil.rmtree(os.path.join(snackPath,'valid'))\n",
    "    # shutil.rmtree(os.path.join(snackPath,'test'))\n",
    "    \n",
    "    # os.makedirs(os.path.join(snackPath,'train'), exist_ok=True)\n",
    "    # os.makedirs(os.path.join(snackPath,'valid'), exist_ok=True)\n",
    "    # os.makedirs(os.path.join(snackPath,'test'), exist_ok=True)\n",
    "    \n",
    "    train_data = shuffleFiles[:trainCount-1]\n",
    "    valid_data = shuffleFiles[trainCount: trainCount + validCount-1]\n",
    "    test_data = shuffleFiles[trainCount + validCount:]\n",
    "    print(f'Data: train {len(train_data)}, valid {len(valid_data)}, test{len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56029a8d-ecf8-40fd-817b-652e6858bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFiles = train_data\n",
    "validFiles = valid_data\n",
    "testFiles = test_data\n",
    "print(len(trainFiles))\n",
    "print(len(validFiles))\n",
    "print(len(testFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab8d73-33d4-40a3-96c1-a60d07c2d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small데이터셋을 위한 폴더를 지정하고 4000장을 나눠서 보관\n",
    "import os\n",
    "\n",
    "sdata = 'sdata\\snack'\n",
    "train_sdata = os.path.join(sdata,'train')\n",
    "valid_sdata = os.path.join(sdata,'valid')\n",
    "test_sdata = os.path.join(sdata,'test')\n",
    "\n",
    "trainValidTestList = ['train', 'valid','test']\n",
    "for trainValidTest in trainValidTestList:\n",
    "    if os.path.exists(os.path.join(sdata, trainValidTest)):\n",
    "        shutil.rmtree(os.path.join(sdata, trainValidTest))\n",
    "\n",
    "os.makedirs(train_sdata, exist_ok=True)\n",
    "os.makedirs(valid_sdata, exist_ok=True)\n",
    "os.makedirs(test_sdata, exist_ok=True)\n",
    "\n",
    "train_sdata_banana = os.path.join(train_sdata,'banana')\n",
    "train_sdata_blueberry = os.path.join(train_sdata,'blueberry')\n",
    "train_sdata_choco = os.path.join(train_sdata,'choco')\n",
    "train_sdata_cone = os.path.join(train_sdata,'cone')\n",
    "train_sdata_gum = os.path.join(train_sdata,'gum')\n",
    "train_sdata_octopus = os.path.join(train_sdata,'octopus')\n",
    "train_sdata_shrimp = os.path.join(train_sdata,'shrimp')\n",
    "os.makedirs(train_sdata_banana, exist_ok=True)\n",
    "os.makedirs(train_sdata_blueberry, exist_ok=True)\n",
    "os.makedirs(train_sdata_choco, exist_ok=True)\n",
    "os.makedirs(train_sdata_cone, exist_ok=True)\n",
    "os.makedirs(train_sdata_gum, exist_ok=True)\n",
    "os.makedirs(train_sdata_octopus, exist_ok=True)\n",
    "os.makedirs(train_sdata_shrimp, exist_ok=True)\n",
    "\n",
    "valid_sdata_banana = os.path.join(valid_sdata,'banana')\n",
    "valid_sdata_blueberry = os.path.join(valid_sdata,'blueberry')\n",
    "valid_sdata_choco = os.path.join(valid_sdata,'choco')\n",
    "valid_sdata_cone = os.path.join(valid_sdata,'cone')\n",
    "valid_sdata_gum = os.path.join(valid_sdata,'gum')\n",
    "valid_sdata_octopus = os.path.join(valid_sdata,'octopus')\n",
    "valid_sdata_shrimp = os.path.join(valid_sdata,'shrimp')\n",
    "os.makedirs(valid_sdata_banana, exist_ok=True)\n",
    "os.makedirs(valid_sdata_blueberry, exist_ok=True)\n",
    "os.makedirs(valid_sdata_choco, exist_ok=True)\n",
    "os.makedirs(valid_sdata_cone, exist_ok=True)\n",
    "os.makedirs(valid_sdata_gum, exist_ok=True)\n",
    "os.makedirs(valid_sdata_octopus, exist_ok=True)\n",
    "os.makedirs(valid_sdata_shrimp, exist_ok=True)\n",
    "\n",
    "test_sdata_banana = os.path.join(test_sdata,'banana')\n",
    "test_sdata_blueberry = os.path.join(test_sdata,'blueberry')\n",
    "test_sdata_choco = os.path.join(test_sdata,'choco')\n",
    "test_sdata_cone = os.path.join(test_sdata,'cone')\n",
    "test_sdata_gum = os.path.join(test_sdata,'gum')\n",
    "test_sdata_octopus = os.path.join(test_sdata,'octopus')\n",
    "test_sdata_shrimp = os.path.join(test_sdata,'shrimp')\n",
    "os.makedirs(test_sdata_banana, exist_ok=True)\n",
    "os.makedirs(test_sdata_blueberry, exist_ok=True)\n",
    "os.makedirs(test_sdata_choco, exist_ok=True)\n",
    "os.makedirs(test_sdata_cone, exist_ok=True)\n",
    "os.makedirs(test_sdata_gum, exist_ok=True)\n",
    "os.makedirs(test_sdata_octopus, exist_ok=True)\n",
    "os.makedirs(test_sdata_shrimp, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea2b3c-f366-43f5-81d0-f5246b300a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 클래스의 이미지 갯수 * 2\n",
    "train_num = len(train_data)\n",
    "valid_num = len(valid_data)\n",
    "test_num  = len(test_data)\n",
    "\n",
    "train_range = [0, train_num-1]\n",
    "valid_range = [train_num, train_num + valid_num -1]\n",
    "test_range  = [train_num + valid_num, train_num + valid_num + test_num-1]\n",
    "\n",
    "print(train_range)\n",
    "print(valid_range)\n",
    "print(test_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f440c4-f059-4101-868c-b05136e5b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 데이터셋 디렉토리 경로 설정 (데이터셋이 저장된 경로로 수정하세요)\n",
    "dataset_dir = 'data\\snack'\n",
    "\n",
    "# 새로운 train, valid, test 디렉토리 생성 경로\n",
    "base_dir = 'sdata\\snack'\n",
    "\n",
    "# 클래스 목록\n",
    "classes = ['banana', 'blueberry', 'choco', 'cone', 'gum', 'octopus', 'shrimp']\n",
    "\n",
    "# 폴더 경로 생성\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "valid_dir = os.path.join(base_dir, 'valid')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# 폴더 생성 함수\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# train, valid, test 폴더 및 각각의 클래스 폴더 생성\n",
    "for cls in classes:\n",
    "    create_dir(os.path.join(train_dir, cls))\n",
    "    create_dir(os.path.join(valid_dir, cls))\n",
    "\n",
    "# 이미지 복사 함수\n",
    "def copy_images(start_idx, end_idx, src_dir, dst_dir, label):\n",
    "    srcFiles = os.listdir(src_dir)\n",
    "    for i in range(start_idx, end_idx + 1):\n",
    "        file_name = f'{label}.{i}.jpg'\n",
    "        # print(f' fileName {file_name}')\n",
    "        src_path = os.path.join(src_dir, srcFiles[i])\n",
    "        # print(f'src {src_path}')\n",
    "        dst_path = os.path.join(dst_dir, file_name)\n",
    "        # print(f'dst {dst_path}')\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "# 클래스별로 train, valid, test 데이터셋 구성\n",
    "for cls in classes:\n",
    "    # train dataset 구성 (0~999)\n",
    "    copy_images(train_range[0], train_range[1], os.path.join(dataset_dir, cls), os.path.join(train_dir, cls), cls)\n",
    "    # print(train_range[0], train_range[1], dataset_dir, os.path.join(train_dir, cls))\n",
    "\n",
    "    # valid dataset 구성 (1000~1249)\n",
    "    copy_images(valid_range[0], valid_range[1], os.path.join(dataset_dir, cls), os.path.join(valid_dir, cls), cls)\n",
    "\n",
    "    # test dataset 구성 (1250~1499)\n",
    "    copy_images(test_range[0], test_range[1], os.path.join(dataset_dir, cls), os.path.join(test_dir, cls), cls)\n",
    "\n",
    "print(\"데이터셋 분할 및 복사가 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3634d3e-e632-4c61-bef1-a6a06aafb895",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirList = [train_sdata_banana, train_sdata_blueberry, train_sdata_choco, train_sdata_cone, train_sdata_gum, train_sdata_octopus, train_sdata_shrimp,\n",
    "           valid_sdata_banana, valid_sdata_blueberry, valid_sdata_choco, valid_sdata_cone, valid_sdata_gum, valid_sdata_octopus, valid_sdata_shrimp,\n",
    "           test_sdata_banana, test_sdata_blueberry, test_sdata_choco, test_sdata_cone, test_sdata_gum, test_sdata_octopus, test_sdata_shrimp]\n",
    "\n",
    "for dir in dirList:\n",
    "    print(len(os.listdir(dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef049767-eac9-4fcd-ac88-f68aa42a08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 복사된 파일들 갯수 확인하기\n",
    "train_banana = os.listdir(os.path.join(train_dir,'banana'))\n",
    "train_blueberry = os.listdir(os.path.join(train_dir,'blueberry'))\n",
    "train_choco = os.listdir(os.path.join(train_dir,'choco'))\n",
    "train_cone = os.listdir(os.path.join(train_dir,'cone'))\n",
    "train_gum = os.listdir(os.path.join(train_dir,'gum'))\n",
    "train_octopus = os.listdir(os.path.join(train_dir,'octopus'))\n",
    "train_shrimp = os.listdir(os.path.join(train_dir,'shrimp'))\n",
    "\n",
    "valid_banana = os.listdir(os.path.join(test_dir,'banana'))\n",
    "valid_blueberry = os.listdir(os.path.join(test_dir,'blueberry'))\n",
    "valid_choco = os.listdir(os.path.join(test_dir,'choco'))\n",
    "valid_cone = os.listdir(os.path.join(test_dir,'cone'))\n",
    "valid_gum = os.listdir(os.path.join(test_dir,'gum'))\n",
    "valid_octopus = os.listdir(os.path.join(test_dir,'octopus'))\n",
    "valid_shrimp = os.listdir(os.path.join(test_dir,'shrimp'))\n",
    "\n",
    "test_banana = os.listdir(os.path.join(test_dir,'banana'))\n",
    "test_blueberry = os.listdir(os.path.join(test_dir,'blueberry'))\n",
    "test_choco = os.listdir(os.path.join(test_dir,'choco'))\n",
    "test_cone = os.listdir(os.path.join(test_dir,'cone'))\n",
    "test_gum = os.listdir(os.path.join(test_dir,'gum'))\n",
    "test_octopus = os.listdir(os.path.join(test_dir,'octopus'))\n",
    "test_shrimp = os.listdir(os.path.join(test_dir,'shrimp'))\n",
    "\n",
    "#위의 6개 폴더 파일 갯수 확인하기\n",
    "print(len(train_banana),len(train_blueberry), len(train_choco),len(train_cone), len(train_gum),len(train_octopus), len(train_shrimp),\n",
    "      len(valid_banana),len(valid_blueberry), len(valid_choco),len(valid_cone), len(valid_gum),len(valid_octopus), len(valid_shrimp),\n",
    "      len(test_banana),len(test_blueberry), len(test_choco),len(test_cone), len(test_gum),len(test_octopus), len(test_shrimp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c39cf5-6bda-412d-8b98-cab7647e9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(os.path.join(os.path.join(train_dir,'banana',train_banana[3])))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea35a6d-2592-4c03-963a-0ead59ac09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "org_size =(256,256)\n",
    "org_size =(224,224)\n",
    "img_size = 224\n",
    "\n",
    "visual_transform = transforms.Compose([\n",
    "    transforms.Resize(org_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(img_size),\n",
    "    # 평균이 0.5 표준편차 0.5 (0~1사이의 실수)\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    #이미지 사이즈가 파일마다 제 각각\n",
    "    transforms.RandomRotation(180),\n",
    "    # transforms.RandomAffine(180, shear=20),\n",
    "    # transforms.RandomPerspective(fill=[255,0,255]),\n",
    "    # transforms.RandomInvert(),\n",
    "    # transforms.RandomPosterize(1.5),\n",
    "    # transforms.RandomSolarize(200.5),\n",
    "    transforms.RandomAdjustSharpness(4),\n",
    "    transforms.RandomAutocontrast(),\n",
    "    # transforms.RandomEqualize(p=0.5),\n",
    "    # transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET),\n",
    "    transforms.Resize(org_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.RandomErasing()\n",
    "    # 평균이 0.5 표준편차 0.5 (0~1사이의 실수)\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(org_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a9cbe-06a8-4073-bdbe-588859c87148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "# Custom ImageFolder to return file name along with image and label\n",
    "class ImageFolderWithFilenames(ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            original_tuple = super(ImageFolderWithFilenames, self).__getitem__(index)\n",
    "            # Get the image file path\n",
    "            path, _ = self.samples[index]\n",
    "            # Return the image, label, and the file name (path)\n",
    "            return original_tuple + (path,)\n",
    "            return data, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data at index {idx}: {e}\")\n",
    "            return None, None  # Return default values to prevent crashes\n",
    "            # Get the original tuple (image, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66233eb1-d15f-4521-a17e-52fb227cbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = os.getcwd() + '/sdata/snack'\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "visual_data = ImageFolder(root=f\"{data_dir}/train\", transform=visual_transform)\n",
    "train_data = ImageFolder(root=f\"{data_dir}/train\", transform=train_transform)\n",
    "valid_data = ImageFolder(root=f\"{data_dir}/valid\", transform=test_transform)\n",
    "test_data = ImageFolderWithFilenames(root=f\"{data_dir}/test\", transform=test_transform)\n",
    "\n",
    "visual_loader = DataLoader(visual_data, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True, persistent_workers=True, timeout=120)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=False, persistent_workers=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=False, persistent_workers=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False, num_workers=1, pin_memory=False, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b32d5-623f-4b71-a162-4810e5985d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train데이터가 2000장이 있는데, 32장씩 데이터를 가져온다.\n",
    "images, labels = next(iter(visual_loader))\n",
    "\n",
    "classes ={0:'banana', 1:'blueberry', 2:'choco', 3:'cone', 4:'gum', 5:'octopus', 6:'shrimp'}\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "for i in range(batch_size):\n",
    "    # 4행 8열\n",
    "    ax = fig.add_subplot(4, 8, i + 1)\n",
    "    ax.set_title(classes[labels[i].item()])\n",
    "    ax.axis('off')\n",
    "    # 컬러 채널 순서를 재정렬\n",
    "    ax.imshow(images[i].permute(1, 2, 0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145083c-fc8c-49c4-8bd3-6f47a3fe0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# EfficientNetV2B0 모델 불러오기 (timm 라이브러리 사용)\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "\n",
    "# 마지막 레이어의 입력 특징 수를 가져옵니다.\n",
    "num_features = model.classifier.in_features\n",
    "num_classes = 7\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 분류기 부분을 Softmax 활성화 함수로 변경\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_features, 256),  # 입력에서 256개의 노드로 연결\n",
    "    torch.nn.ReLU(),                     # 활성화 함수 ReLU\n",
    "    torch.nn.Linear(256, num_classes)    # 최종 분류 클래스의 수에 맞게 출력 노드 수 설정\n",
    ").to(device)\n",
    "\n",
    "# 모델을 GPU 또는 CPU에 할당\n",
    "model = model.to(device)\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘 설정\n",
    "criterion = nn.CrossEntropyLoss()  # 이진 크로스 엔트로피 손실 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# 학습율을 조정\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd72d181-e0a5-4c86-b4a5-edc00e8e3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50, mobilenet_v3_large\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "isResnet50 = False\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ImageNet에 사전학습된 모델을 가져와서 학습(Train)\n",
    "# 분류기는 제외하고 특징 추출기만 사용\n",
    "\n",
    "if isResnet50:\n",
    "    model = resnet50(pretrained=True).to(device)\n",
    "else:\n",
    "    model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e044d4-2944-4ba1-a6e1-8f7c957ed3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=True).to(device)\n",
    "\n",
    "# 특징 추출기는 그대로 사용하고, 분류기만 2개짜리로 변경\n",
    "# fc -> 분류기 in_features -> 분류기의 입력\n",
    "num_features = model.fc.in_features\n",
    "\n",
    "# num_classes: 분류해야 할 클래스 수 (예: 3개의 클래스)\n",
    "num_classes = 7\n",
    "\n",
    "# 분류기 부분을 Softmax 활성화 함수로 변경\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_features, 256),  # 입력에서 256개의 노드로 연결\n",
    "    torch.nn.ReLU(),                     # 활성화 함수 ReLU\n",
    "    torch.nn.Linear(256, num_classes)    # 최종 분류 클래스의 수에 맞게 출력 노드 수 설정\n",
    ").to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2599a-6ac3-428f-9b5e-b00b10ef7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 학습 진행\n",
    "def fit(model, criterion, optimizer, epochs, train_loader, valid_loader, resume=False):\n",
    "\n",
    "    if resume==False:\n",
    "        # 그래프로 출력하기 위한 리스트\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        valid_losses = []\n",
    "        valid_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        # 훈련 모드\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        for train_x, train_y in tqdm(train_loader):\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_x)\n",
    "\n",
    "            # softmax + loss\n",
    "            loss = criterion(outputs, train_y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # batch마다 train_loss를 누적\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_correct += (predicted == train_y).sum().item()\n",
    "\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        valid_correct = 0\n",
    "\n",
    "        for valid_x, valid_y in valid_loader:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                valid_x, valid_y = valid_x.to(device), valid_y.to(device)\n",
    "                outputs = model(valid_x)\n",
    "                loss = criterion(outputs, valid_y)\n",
    "                valid_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                valid_correct += (predicted == valid_y).sum().item()\n",
    "\n",
    "        train_acc = train_correct/len(train_loader.dataset)\n",
    "        valid_acc = valid_correct/len(valid_loader.dataset)\n",
    "\n",
    "        print(f'{time.time() - start:.3f}sec : [Epoch {epoch+1}/{epochs}] -> train loss: {train_loss/len(train_loader):.4f}, train acc: {train_acc*100:.3f}% / valid loss: {valid_loss/len(valid_loader):.4f}, valid acc: {valid_acc*100:.3f}%')\n",
    "\n",
    "        train_losses.append(train_loss/len(train_loader))\n",
    "        train_accuracies.append(train_acc)\n",
    "        valid_losses.append(valid_loss/len(valid_loader))\n",
    "        valid_accuracies.append(valid_acc)\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "        # 현재 learning_rate 값을 읽어올 수 있다.\n",
    "        now_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'learning_rate {epoch+1}: {now_lr:.8f}')\n",
    "\n",
    "        # EarlyStopping을 호출하여 학습 중단 여부 확인\n",
    "        early_stopping(valid_loss, model)\n",
    "\n",
    "        # 학습 중단 조건을 충족하면 break\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "        print('-' * 100)\n",
    "\n",
    "    plt.plot(train_losses, label='loss')\n",
    "    plt.plot(train_accuracies, label='accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('train loss and accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(valid_losses, label='loss')\n",
    "    plt.plot(valid_accuracies, label='accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('valid loss and accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76abc5d-f6aa-42fb-b225-b36cd0d82ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): 성능 개선이 없을 때 몇 번의 에포크까지 기다릴지.\n",
    "            verbose (bool): True일 경우 개선될 때마다 메시지 출력.\n",
    "            delta (float): 성능 개선으로 간주될 최소 변화량.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        # 처음에 호출됐을때는 best_score가 None이라서 초기값을 설정\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        # 지금까지의 best_score와 현재 score를 비교\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            # patience값이 모두 충족했을때, 종료조건이 만족될때\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''검증 손실이 감소하면 모델을 저장합니다.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), self.path)  # 모델 상태 저장\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# EarlyStopping 인스턴스 생성 (patience=10)\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True, path='resnet50_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23085f8c-e40b-41e2-b90e-47d48a4fb15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "LR = 0.00001\n",
    "EPOCHS = 25\n",
    "# 손실함수\n",
    "# 신경망에 sigmoid를 썼으므로 BCELoss()사용\n",
    "# 만약 신경망에 softmax를 썼다면 CrossEntropyLoss()사용\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "# optimizer정의\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# scheduler 추가\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# 학습\n",
    "fit(model, criterion, optimizer, EPOCHS, train_loader, valid_loader, resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45acae66-a607-4936-b804-87c85a38ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# 테스트 데이터에서 성능을 평가하고, 예측이 잘못된 이미지를 시각화하는 함수\n",
    "def evaluate_and_visualize(model, test_loader, criterion):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    #incorrect_images = []\n",
    "    incorrect_labels = []\n",
    "    incorrect_preds = []\n",
    "    incorrect_filenames = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_x, test_y, filenames in tqdm(test_loader):\n",
    "            test_x, test_y = test_x.to(device), test_y.to(device)\n",
    "            outputs = model(test_x)\n",
    "            loss = criterion(outputs, test_y)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # 정확도 계산\n",
    "            test_correct += (predicted == test_y).sum().item()\n",
    "            test_total += test_y.size(0)\n",
    "\n",
    "            if predicted != test_y:\n",
    "                incorrect_labels.append(test_y.cpu().numpy())\n",
    "                incorrect_preds.append(predicted.cpu().numpy())\n",
    "                incorrect_filenames.append(filenames[0])\n",
    "\n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(\"test_correct:{}, test_total:{}\".format(test_correct, test_total))\n",
    "    print(f'Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "    print(len(incorrect_filenames))\n",
    "\n",
    "    # 예측이 잘못된 이미지 시각화\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    for i in range(min(16, len(incorrect_filenames))):\n",
    "        ax = fig.add_subplot(4, 4, i + 1)\n",
    "        image = Image.open(incorrect_filenames[i])\n",
    "        ax.imshow(image)\n",
    "        label_dict = {0:'banana', 1:'blueberry', 2:'choco', 3:'cone', 4:'gum', 5:'octopus', 6:'shrimp'}\n",
    "        true_label = label_dict[incorrect_labels[i]]\n",
    "        pred_label = label_dict[incorrect_preds[i]]\n",
    "        # true_label = label_dict[] if incorrect_labels[i] == 1 else 'blueberry'\n",
    "        # pred_label = label_dict[] if incorrect_preds[i] == 1 else 'blueberry'\n",
    "        filename = os.path.basename(incorrect_filenames[i])\n",
    "        ax.set_title(f'Pred: {pred_label}, Label: {true_label}, {filename}', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3067d9-2e58-47a0-9271-67c39d2e1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터를 통해 성능 평가 및 잘못된 예측 시각화\n",
    "evaluate_and_visualize(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4022c160-d5b0-4a04-b6c0-ffdfd927b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Function to classify and display an image\n",
    "def classify_and_show_image(model, image_path, device='cpu'):\n",
    "    # Assuming the model is already loaded and `label_dict` contains the mapping of class indices to labels\n",
    "    label_dict = {0: 'banana', 1: 'blueberry', 2: 'choco', 3: 'cone', 4: 'gum', 5: 'octopus', 6: 'shrimp'}\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Define preprocessing transformations (these should match the ones used during training)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to the input size expected by the model\n",
    "        transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
    "    ])\n",
    "    \n",
    "    # Apply transformations to the image\n",
    "    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # Make sure the model is in evaluation mode\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "    \n",
    "    # Get the predicted label\n",
    "    predicted_label = label_dict[predicted_class.item()]\n",
    "    true_label = os.path.basename(image_path).split(\".\")[0]\n",
    "\n",
    "    # Plot the image and show prediction\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Predicted: {predicted_label}, True Label: {true_label}\")\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "baseDir = os.getcwd()\n",
    "uploadDir = os.path.join(baseDir, 'upload', 'snack')\n",
    "# Detect if a GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for file in os.listdir(uploadDir): \n",
    "    image_path = os.path.join(uploadDir, file)  # Replace with the path to your image\n",
    "    # Replace with the true label of the image\n",
    "    classify_and_show_image(model, image_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3099375-26c5-40b2-80a4-785dd1eeb3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss_total = 0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labelsx)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss_total += loss.item()\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    val_loss = loss_total / len(data_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "    val_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    return val_loss, val_acc, val_f1, all_preds, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aedd683-69f3-4570-9412-ee0a94e10420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['banana', 'blueberry', 'choco', 'cone', 'gum', 'octopus', 'shrimp']\n",
    "\n",
    "def plot_confusion_matrix(labels, predictions, class_names):\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "_, _, _, preds, labels = evaluate_model(model, test_loader)\n",
    "plot_confusion_matrix(labels, preds, class_anmes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca998bc0-9110-483d-8373-f899e8472c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
